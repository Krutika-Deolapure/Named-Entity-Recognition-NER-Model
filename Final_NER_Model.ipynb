{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POo1E4tZq-Ll",
        "outputId": "8042c82e-eba6-44f4-d821-84f1a2b84808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in training data: 14041\n",
            "Number of sentences in test data: 3453\n",
            "Number of sentences in validation data: 3250\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Read the files\n",
        "train_file = '/content/drive/MyDrive/coNLL-2003/train.txt'\n",
        "test_file = '/content/drive/MyDrive/coNLL-2003/test.txt'\n",
        "valid_file = '/content/drive/MyDrive/coNLL-2003/valid.txt'\n",
        "\n",
        "# Load the datasets\n",
        "def load_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        sentence = []\n",
        "        for line in file:\n",
        "            if line.startswith(\"-DOCSTART-\") or line == \"\\n\":\n",
        "                if sentence:\n",
        "                    data.append(sentence)\n",
        "                    sentence = []\n",
        "                continue\n",
        "            splits = line.split()\n",
        "            sentence.append((splits[0], splits[-1]))\n",
        "        if sentence:\n",
        "            data.append(sentence)\n",
        "    return data\n",
        "\n",
        "train_data = load_data(train_file)\n",
        "test_data = load_data(test_file)\n",
        "valid_data = load_data(valid_file)\n",
        "\n",
        "# Perform EDA\n",
        "print(f\"Number of sentences in training data: {len(train_data)}\")\n",
        "print(f\"Number of sentences in test data: {len(test_data)}\")\n",
        "print(f\"Number of sentences in validation data: {len(valid_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Load SpaCy's English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.lemma_ for token in doc if token.text not in STOP_WORDS and token.is_alpha]\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the sentences\n",
        "def preprocess_sentences(data):\n",
        "    preprocessed_data = []\n",
        "    for sentence in data:\n",
        "        preprocessed_sentence = []\n",
        "        for token, label in sentence:\n",
        "            preprocessed_sentence.append((token.lower(), label))\n",
        "        preprocessed_data.append(preprocessed_sentence)\n",
        "    return preprocessed_data\n",
        "\n",
        "train_data = preprocess_sentences(train_data)\n",
        "test_data = preprocess_sentences(test_data)\n",
        "valid_data = preprocess_sentences(valid_data)"
      ],
      "metadata": {
        "id": "p-mOttwOrlRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_spacy_format(data):\n",
        "    spacy_data = []\n",
        "    for sentence in data:\n",
        "        tokens = [token for token, label in sentence]\n",
        "        entities = []\n",
        "        start = 0\n",
        "        for token, label in sentence:\n",
        "            if label != 'O':\n",
        "                entity_type = label.split('-')[1]\n",
        "                end = start + len(token)\n",
        "                entities.append((start, end, entity_type))\n",
        "            start += len(token) + 1  # +1 for the space\n",
        "        spacy_data.append((\" \".join(tokens), {\"entities\": entities}))\n",
        "    return spacy_data\n",
        "\n",
        "train_data_spacy = convert_to_spacy_format(train_data)\n",
        "test_data_spacy = convert_to_spacy_format(test_data)\n",
        "valid_data_spacy = convert_to_spacy_format(valid_data)\n",
        "\n",
        "# Check the format of one training example\n",
        "print(train_data_spacy[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ye_uoqSr4iu",
        "outputId": "6762e33f-192f-4fe4-e112-9934c836caf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('eu rejects german call to boycott british lamb .', {'entities': [(0, 2, 'ORG'), (11, 17, 'MISC'), (34, 41, 'MISC')]})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "\n",
        "# Create a blank model\n",
        "ner_model = spacy.blank('en')\n",
        "\n",
        "# Adding NER pipeline\n",
        "ner = ner_model.add_pipe('ner')\n",
        "\n",
        "# Adding labels to the NER pipeline\n",
        "for _, annotations in train_data_spacy:\n",
        "    for ent in annotations.get('entities'):\n",
        "        ner.add_label(ent[2])\n",
        "\n",
        "# Training the NER model\n",
        "optimizer = ner_model.begin_training()\n",
        "for i in range(10):\n",
        "    losses = {}\n",
        "    for text, annotations in train_data_spacy:\n",
        "        example = Example.from_dict(ner_model.make_doc(text), annotations)\n",
        "        ner_model.update([example], sgd=optimizer, losses=losses)\n",
        "    print(f\"Losses at iteration {i}: {losses}\")\n",
        "\n",
        "# Save the trained model\n",
        "ner_model.to_disk(\"ner_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjg6yoy33Sge",
        "outputId": "1188961f-93de-40e5-ce39-24252a5a18ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses at iteration 0: {'ner': 25279.164906708254}\n",
            "Losses at iteration 1: {'ner': 16639.567506198935}\n",
            "Losses at iteration 2: {'ner': 12950.233830539863}\n",
            "Losses at iteration 3: {'ner': 10973.965105321333}\n",
            "Losses at iteration 4: {'ner': 9624.510969091}\n",
            "Losses at iteration 5: {'ner': 8797.047597918116}\n",
            "Losses at iteration 6: {'ner': 8233.231268975806}\n",
            "Losses at iteration 7: {'ner': 7556.344762219701}\n",
            "Losses at iteration 8: {'ner': 7226.671152668854}\n",
            "Losses at iteration 9: {'ner': 7051.985375567793}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the trained model\n",
        "ner_model = spacy.load(\"ner_model\")\n",
        "\n",
        "# Evaluation function to map character positions to token indices\n",
        "def evaluate_model(ner_model, data):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for text, annotations in data:\n",
        "        doc = ner_model(text)\n",
        "        true_entities = annotations['entities']\n",
        "        pred_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "\n",
        "        # Created a list of tokens and labels\n",
        "        tokens = text.split()\n",
        "        true_labels = ['O'] * len(tokens)\n",
        "        pred_labels = ['O'] * len(tokens)\n",
        "\n",
        "        # Map character positions to token indices for true entities\n",
        "        for start_char, end_char, label in true_entities:\n",
        "            start_idx = None\n",
        "            end_idx = None\n",
        "            char_pos = 0\n",
        "            for i, token in enumerate(tokens):\n",
        "                token_start = char_pos\n",
        "                token_end = char_pos + len(token)\n",
        "                if start_idx is None and token_start <= start_char < token_end:\n",
        "                    start_idx = i\n",
        "                if token_start < end_char <= token_end:\n",
        "                    end_idx = i + 1\n",
        "                    break\n",
        "                char_pos += len(token) + 1\n",
        "\n",
        "            if start_idx is not None and end_idx is not None:\n",
        "                for i in range(start_idx, end_idx):\n",
        "                    true_labels[i] = label\n",
        "\n",
        "        # Map character positions to token indices for predicted entities\n",
        "        for start_char, end_char, label in pred_entities:\n",
        "            start_idx = None\n",
        "            end_idx = None\n",
        "            char_pos = 0\n",
        "            for i, token in enumerate(tokens):\n",
        "                token_start = char_pos\n",
        "                token_end = char_pos + len(token)\n",
        "                if start_idx is None and token_start <= start_char < token_end:\n",
        "                    start_idx = i\n",
        "                if token_start < end_char <= token_end:\n",
        "                    end_idx = i + 1\n",
        "                    break\n",
        "                char_pos += len(token) + 1\n",
        "\n",
        "            if start_idx is not None and end_idx is not None:\n",
        "                for i in range(start_idx, end_idx):\n",
        "                    pred_labels[i] = label\n",
        "\n",
        "        y_true.extend(true_labels)\n",
        "        y_pred.extend(pred_labels)\n",
        "\n",
        "    print(classification_report(y_true, y_pred, zero_division=1))\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(ner_model, test_data_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0_TVGgs3dJ-",
        "outputId": "8de21246-025c-4948-ff0b-992665a9675d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.73      0.86      0.79      1925\n",
            "        MISC       0.64      0.76      0.69       918\n",
            "           O       0.98      0.95      0.97     38323\n",
            "         ORG       0.63      0.59      0.61      2496\n",
            "         PER       0.63      0.87      0.73      2773\n",
            "\n",
            "    accuracy                           0.92     46435\n",
            "   macro avg       0.72      0.81      0.76     46435\n",
            "weighted avg       0.93      0.92      0.92     46435\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.util import minibatch, compounding\n",
        "import random\n",
        "\n",
        "# Hyperparameters\n",
        "n_iter = 20\n",
        "batch_sizes = compounding(4.0, 32.0, 1.001)\n",
        "\n",
        "# Training the NER model with hyperparameter tuning\n",
        "optimizer = ner_model.begin_training()\n",
        "for i in range(n_iter):\n",
        "    random.shuffle(train_data_spacy)\n",
        "    losses = {}\n",
        "    batches = minibatch(train_data_spacy, size=batch_sizes)\n",
        "    for batch in batches:\n",
        "        for text, annotations in batch:\n",
        "            example = Example.from_dict(ner_model.make_doc(text), annotations)\n",
        "            ner_model.update([example], sgd=optimizer, losses=losses)\n",
        "    print(f\"Losses at iteration {i}: {losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-Nr9F7u3lTz",
        "outputId": "80558cee-e78c-4dab-d67f-5b7a9826944a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses at iteration 0: {'ner': 25112.164629945495}\n",
            "Losses at iteration 1: {'ner': 15944.30416615865}\n",
            "Losses at iteration 2: {'ner': 12657.166979519201}\n",
            "Losses at iteration 3: {'ner': 10680.233803815318}\n",
            "Losses at iteration 4: {'ner': 9392.75999691699}\n",
            "Losses at iteration 5: {'ner': 8499.091049969538}\n",
            "Losses at iteration 6: {'ner': 7794.64563372191}\n",
            "Losses at iteration 7: {'ner': 7624.673642367829}\n",
            "Losses at iteration 8: {'ner': 7328.527135310303}\n",
            "Losses at iteration 9: {'ner': 7133.846433882183}\n",
            "Losses at iteration 10: {'ner': 6659.078680311733}\n",
            "Losses at iteration 11: {'ner': 6540.233494758112}\n",
            "Losses at iteration 12: {'ner': 6158.41653732479}\n",
            "Losses at iteration 13: {'ner': 5989.326527768521}\n",
            "Losses at iteration 14: {'ner': 5877.946050942893}\n",
            "Losses at iteration 15: {'ner': 5845.147243482898}\n",
            "Losses at iteration 16: {'ner': 5773.716715156144}\n",
            "Losses at iteration 17: {'ner': 5588.3435025619265}\n",
            "Losses at iteration 18: {'ner': 5678.182558758138}\n",
            "Losses at iteration 19: {'ner': 5628.958919713241}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the optimized model\n",
        "evaluate_model(ner_model, test_data_spacy)"
      ],
      "metadata": {
        "id": "O93iXV4n3rqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "814601d9-5870-42c5-9acd-805b96b0d35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.86      0.80      0.83      1925\n",
            "        MISC       0.75      0.67      0.71       918\n",
            "           O       0.96      0.99      0.97     38323\n",
            "         ORG       0.79      0.65      0.71      2496\n",
            "         PER       0.92      0.71      0.80      2773\n",
            "\n",
            "    accuracy                           0.94     46435\n",
            "   macro avg       0.86      0.76      0.80     46435\n",
            "weighted avg       0.94      0.94      0.94     46435\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model to a file using Joblib\n",
        "joblib.dump(ner_model, \"optimized_ner_model.joblib\")"
      ],
      "metadata": {
        "id": "chaCC8tS3vZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc09831-ceae-4cb1-d5cd-a98f3ab12c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['optimized_ner_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the model to a file using pickle\n",
        "model_path = \"optimized_ner_model.pkl\"\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(ner_model, f)"
      ],
      "metadata": {
        "id": "pCt7vEIUvbhX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}